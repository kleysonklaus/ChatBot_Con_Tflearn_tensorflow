{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "words = []\n",
    "labels = [] # TAG\n",
    "docs_x = [] # Frase de entrada\n",
    "docs_y = [] # TAG Resultado\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nltk.word_tokenize(pattern) # devuelve una lista con las diferentes palabras\n",
    "        words.extend(wrds)                 # extiende la lista de words, haciendo que sea solo una lista\n",
    "        docs_x.append( wrds )\n",
    "        docs_y.append( intent[\"tag\"] )\n",
    "        \n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "words = [ stemmer.stem( w.lower() ) for w in words if w != \"?\"]\n",
    "words = sorted ( list( set( words ) ) )\n",
    "\n",
    "labels = sorted( labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'acc', 'adio', 'ah', 'ahor', 'alguy', 'animac', 'animada', 'asdfass', 'asdfsdf', 'aventur', 'ayudam', 'ayudarm', 'aã±o', 'aã±os', 'buen', 'bye', 'chao', 'chist', 'cienc', 'comed', 'como', 'cont', 'crim', 'cual', 'cuanto', 'cuentam', 'de', 'debo', 'dia', 'diseã±ado', 'dormir', 'dram', 'duerm', 'edad', 'en', 'er', 'es', 'escribir', 'esta', 'fantas', 'ficc', 'fuist', 'hac', 'hago', 'has', 'hasm', 'hay', 'hecho', 'hol', 'hum', 'infantil', 'ir', 'las', 'listado', 'llam', 'llama', 'llamart', 'luego', 'mam', 'me', 'mi', 'misterio', 'muestram', 'no', 'nombr', 'nueva', 'objetivo', 'par', 'pas', 'pel', 'pelicula', 'por', 'porqu', 'pued', 'pusieron', 'que', 'qui', 'quiero', 'reir', 'rom', 'sab', 'sdfsadfasdfasdf', 'se', 'skynet', 'sobr', 'su', 'suspenso', 'tal', 'te', 'ten', 'tenga', 'tengo', 'ter', 'tien', 'toda', 'tu', 'un', 'veo', 'voy', 'y']\n",
      "==============================\n",
      "['accion', 'adios', 'animacion', 'aventura', 'chiste', 'ciencia ficcion', 'comedia', 'crimen', 'drama', 'edad', 'fail', 'fantasia']\n",
      "==============================\n",
      "[['Hola', '!'], ['Como', 'estas', '?'], ['Hay', 'alguien', 'ahi', '?'], ['hola'], ['buen', 'dia'], ['que', 'pasa', '?'], ['que', 'tal', '?'], ['Te', 'veo', 'luego'], ['adios'], ['me', 'tengo', 'que', 'ir'], ['ten', 'un', 'buen', 'dia'], ['mi', 'mama', 'me', 'llama']]\n",
      "==============================\n",
      "['saludo', 'saludo', 'saludo', 'saludo', 'saludo', 'saludo', 'saludo', 'adios', 'adios', 'adios', 'adios', 'adios', 'adios', 'adios', 'adios', 'edad', 'edad', 'edad', 'nombre', 'nombre', 'nombre', 'nombre', 'nombre', 'porque_ese_nombre', 'porque_ese_nombre', 'porque_ese_nombre', 'horas', 'horas', 'horas', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'help', 'chiste', 'chiste', 'chiste', 'chiste', 'chiste', 'listado', 'listado', 'listado', 'listado', 'listado', 'listado', 'listado', 'listado', 'animacion', 'animacion', 'animacion', 'animacion', 'animacion', 'animacion', 'animacion', 'accion', 'accion', 'accion', 'accion', 'accion', 'accion', 'accion', 'infantil', 'infantil', 'infantil', 'infantil', 'infantil', 'infantil', 'infantil', 'comedia', 'comedia', 'comedia', 'comedia', 'comedia', 'comedia', 'comedia', 'terror', 'terror', 'terror', 'terror', 'terror', 'terror', 'terror', 'ciencia ficcion', 'ciencia ficcion', 'ciencia ficcion', 'ciencia ficcion', 'ciencia ficcion', 'ciencia ficcion', 'ciencia ficcion', 'aventura', 'aventura', 'aventura', 'aventura', 'aventura', 'aventura', 'aventura', 'suspenso', 'suspenso', 'suspenso', 'suspenso', 'suspenso', 'suspenso', 'suspenso', 'romance', 'romance', 'romance', 'romance', 'romance', 'romance', 'romance', 'fantasia', 'fantasia', 'fantasia', 'fantasia', 'fantasia', 'fantasia', 'fantasia', 'drama', 'drama', 'drama', 'drama', 'drama', 'drama', 'drama', 'crimen', 'crimen', 'crimen', 'crimen', 'crimen', 'crimen', 'crimen', 'crimen', 'misterio', 'misterio', 'misterio', 'misterio', 'misterio', 'misterio', 'misterio', 'fail', 'fail', 'fail']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(\"==============================\")\n",
    "print(labels[:12])\n",
    "print(\"==============================\")\n",
    "print(docs_x[:12])\n",
    "print(\"==============================\")\n",
    "print(docs_y)\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accion',\n",
       " 'adios',\n",
       " 'animacion',\n",
       " 'aventura',\n",
       " 'chiste',\n",
       " 'ciencia ficcion',\n",
       " 'comedia',\n",
       " 'crimen',\n",
       " 'drama',\n",
       " 'edad',\n",
       " 'fail',\n",
       " 'fantasia',\n",
       " 'help',\n",
       " 'horas',\n",
       " 'infantil',\n",
       " 'listado',\n",
       " 'misterio',\n",
       " 'nombre',\n",
       " 'porque_ese_nombre',\n",
       " 'romance',\n",
       " 'saludo',\n",
       " 'suspenso',\n",
       " 'terror']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# creando numeros que representen a las palabras\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [ 0 for _ in range (len(labels)) ]\n",
    "print (out_empty)\n",
    "\n",
    "for x, doc in enumerate( docs_x ):\n",
    "    bag = []\n",
    "    wrds = [ stemmer.stem(w) for w in doc ]\n",
    "    \n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    output_row = out_empty[:]\n",
    "    output_row [ labels.index( docs_y[x] ) ]  = 1\n",
    "    \n",
    "    training.append( bag )\n",
    "    output.append( output_row )\n",
    "    \n",
    "    \n",
    "training = numpy.array( training )\n",
    "output = numpy.array( output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', '!']\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print (docs_x[0])\n",
    "print(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accion', 'adios', 'animacion', 'aventura', 'chiste', 'ciencia ficcion', 'comedia', 'crimen', 'drama', 'edad', 'fail', 'fantasia', 'help', 'horas', 'infantil', 'listado', 'misterio', 'nombre', 'porque_ese_nombre', 'romance', 'saludo', 'suspenso', 'terror']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print (labels)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Toshiba\\PROYECTOS\\#ProcesamientoLenguajeNatural\\project_chatbot\\save_model\\model.tflearn\n",
      "recargado!\n"
     ]
    }
   ],
   "source": [
    "# para eliminar todos los ajustes anteriores para restablecer , define un grapho vacio\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data( shape=[None,len( training[0] )] )\n",
    "net = tflearn.fully_connected(net,280)\n",
    "net = tflearn.fully_connected(net,200)\n",
    "net = tflearn.fully_connected(net,300)\n",
    "net = tflearn.fully_connected(net,90)\n",
    "net = tflearn.fully_connected(net,len(output[0]), activation = \"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load(\"save_model/model.tflearn\")\n",
    "    print (\"recargado!\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch = 500 , batch_size = 8, show_metric = True)\n",
    "    model.save(\"save_model/model.tflearn\")\n",
    "\n",
    "# model.fit(training, output, n_epoch = 100 , batch_size = 8, show_metric = True)\n",
    "# model.save(\"save_model/model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words (s, words):\n",
    "    bag = [0 for _ in range ( len(words) )]\n",
    "    \n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem( word.lower() ) for word in s_words ]\n",
    "    \n",
    "    for se  in s_words:\n",
    "        for i,w in enumerate (words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "                \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_peliculas(tag):\n",
    "    peliculas = json.load(open('peliculas.json', 'r', encoding='utf-8'))\n",
    "    \n",
    "    if tag == \"listado\":\n",
    "        for i in peliculas[\"peliculas\"]:\n",
    "            print(i[\"year\"],\" \",i[\"pelicula\"])\n",
    "            \n",
    "    if tag in labels:\n",
    "        # if para buscar por genero\n",
    "        if tag not in peliculas['peliculas']:\n",
    "            \n",
    "            for datos in peliculas[\"peliculas\"]:\n",
    "                for gen in datos['genero']:\n",
    "                    if tag.lower() == gen.lower():\n",
    "                        print(gen,\" \", datos[\"pelicula\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama   Historia de un matrimonio\n",
      "Drama   Estafadoras de Wall Street\n",
      "Drama   It capítulo dos\n",
      "drama   El jilguero\n",
      "Drama   En la hierba alta\n",
      "Drama   Cada día\n",
      "Drama   El Rey Arturo: La Leyenda de la Espada\n",
      "Drama   Estación Zombie\n",
      "Drama   La llegada\n",
      "Drama   La leyenda de Tarzán\n"
     ]
    }
   ],
   "source": [
    "verificar_peliculas(\"drama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> you: asefasfd\n",
      " === IA:  no se nada de eso, pregunta otra cosa\n",
      " >>> you: etrhhrtnfg\n",
      " === IA:  podrias escribir mas claro?\n",
      " >>> you: q u iero peliculas de terror\n",
      " === IA:  genial, aqui tengo todas las peliculas de tipo terror\n",
      "Terror   It capítulo dos\n",
      "Terror   Boda sangrienta\n",
      "Terror   En la hierba alta\n",
      "Terror   El despertar de los muertos vivientes\n",
      "Terror   Feliz día de tu muerte\n",
      "Terror   Life: Vida Inteligente\n",
      "Terror   Estación Zombie\n",
      " >>> you: adios\n",
      " === IA:  hasta la vista Baby!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input (\" >>> you: \")\n",
    "    \n",
    "    resp = model.predict([ bag_of_words(inp,words) ])[0] # retorna la primera linea y solo es una simple lista\n",
    "#     print (resp)\n",
    "    resp_index = numpy.argmax(resp)\n",
    "#     print (resp [resp_index])\n",
    "#     print (resp_index)\n",
    "    \n",
    "    if resp [resp_index] > 0.70:\n",
    "        tag = labels[resp_index]\n",
    "    else:\n",
    "        tag = \"fail\"\n",
    "        \n",
    "    for tg in data[\"intents\"]:\n",
    "        if tg[\"tag\"] == tag:\n",
    "            responses = tg[\"responses\"]\n",
    "\n",
    "#     print (tag)\n",
    "#     print(responses)\n",
    "    print ( \" === IA: \",random.choice(responses) )\n",
    "    verificar_peliculas(tag)\n",
    "    if tag == \"adios\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
